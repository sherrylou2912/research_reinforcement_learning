{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Reward Process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [\n",
    "    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],\n",
    "    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "] # example transition matrix\n",
    "\n",
    "P = np.array(P)\n",
    "\n",
    "rewards = [-1, -2, -2, 10, 1, 0]\n",
    "gamma = 0.5 # discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return starting from state 1: -2.5\n"
     ]
    }
   ],
   "source": [
    "rewards = [-1, -2, -2, 10, 1, 0]\n",
    "gamma = 0.5 # discount factor\n",
    "\n",
    "def compute_return(start_index, chain, gamma):\n",
    "    G = 0\n",
    "    for i in reversed(range(start_index, len(chain))):\n",
    "        G = gamma*G + rewards[chain[i] - 1]\n",
    "    return G\n",
    "\n",
    "chain = [1,2,3,6]\n",
    "start_index = 0\n",
    "G = compute_return(start_index, chain, gamma)\n",
    "print(\"return starting from state 1:\", G)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## value function \n",
    "\n",
    "$V(s) = E[G_t|S_t = s] = E[R_t + \\gamma V(S_{t+1})|S_t = s]$\n",
    "\n",
    "$V = R + \\gamma P V$\n",
    "\n",
    "$V = (I - \\gamma P)^{-1} R$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve bellman equation: analytic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value for each states\n",
      " [[-2.01950168]\n",
      " [-2.21451846]\n",
      " [ 1.16142785]\n",
      " [10.53809283]\n",
      " [ 3.58728554]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def compute(P, rewards, gamma, states_numer):\n",
    "    rewards = np.array(rewards).reshape(-1,1)\n",
    "    value = np.dot(np.linalg.inv(np.eye(states_numer, states_numer) - gamma*P), rewards)\n",
    "    return value\n",
    "\n",
    "V = compute(P,rewards,gamma,6)\n",
    "print(\"Value for each states\\n\", V)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov decision process "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy: $\\pi = P(A_t = a|S_t = s) $ \n",
    "\n",
    "state-value function: $V^\\pi (s) = E_\\pi [G_t |S_t = s]$ \n",
    "\n",
    "action-value function: $Q^\\pi (s,a) = E_\\pi [G_t | S_t = s, A_t = a]$\n",
    "\n",
    "Remark: under fixed policy $\\pi$ we have following relation between state value and action value:\n",
    "\n",
    "$$V^\\pi = \\sum_{a \\in A} \\pi(a|s)Q^\\pi (s,a)$$\n",
    "\n",
    "$$Q^\\pi (s,a) = r(s,a) + \\gamma \\sum_{s' \\in S} P(s' | s, a) V^\\pi (s') $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Expectation Equation \n",
    "\n",
    "substitude above relation between state-value function and action-value function into Bellman Equation we get following **Bellman Expectation Equation** for value function:\n",
    "\n",
    "\\begin{aligned}\n",
    "V^\\pi(s) &= \\mathbb{E}_\\pi \\left[ R_t + \\gamma V^\\pi(S_{t+1}) \\mid S_t = s \\right] \\\\\n",
    "         &= \\sum_{a \\in A} \\pi(a|s) \\left( r(s,a) + \\gamma \\sum_{s' \\in S} p(s'|s,a) V^\\pi(s') \\right)\n",
    "\\end{aligned}\n",
    "\n",
    "\\begin{aligned}\n",
    "Q^\\pi(s,a) &= E_\\pi[R_t + \\gamma Q^\\pi (S_{t+1}, A_{t+1}) | S_t = s, A_t = a]\\\\\n",
    "& = r(s,a) + \\gamma \\sum_{s' \\in S} p(s'|s,a) \\sum_{a' \\in A} \\pi(a'|s') Q^\\pi(s'|a')\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = ['s1', 's2', 's3','s4','s5']\n",
    "A = ['keep s1', 'go to s1', 'go to s2', 'go to s3', 'go to s4', 'go to s5', 'random transit']\n",
    "P = {\n",
    "    \"s1-keep s1-s1\" : 1.0,\n",
    "    \"s1-go to s2-s2\" : 1.0,\n",
    "    \"s2-go to s1-s1\" : 1.0,\n",
    "    \"s2-go to s3-s3\": 1.0,\n",
    "    \"s3-go to s4-s4\": 1.0,\n",
    "    \"s3-go to s5-s5\":1.0,\n",
    "    \"s4-go to s5-s5\": 1.0,\n",
    "    \"s4-random transit-s2\": 0.2,\n",
    "    \"s4-random transit-s3\": 0.4,\n",
    "    \"s4-random transit-s4\": 0.4, \n",
    "}\n",
    "\n",
    "R = {\n",
    "    \"s1-keep s1\": -1,\n",
    "    \"s1-go to s2\": 0,\n",
    "    \"s2-go to s1\": -1,\n",
    "    \"s2-go to s3\": -2,\n",
    "    \"s3-go to s4\": -2,\n",
    "    \"s3-go to s5\": 0,\n",
    "    \"s4-go to s5\": 10,\n",
    "    \"s4-random transit\": 1,\n",
    "\n",
    "}\n",
    "\n",
    "gamma = 0.5\n",
    "MDP = (S,A,P,R,gamma)\n",
    "\n",
    "Pi_1 = {\n",
    "    \"s1-keep s1\": 0.5,\n",
    "    \"s1-go to s2\": 0.5,\n",
    "    \"s2-go to s1\": 0.5,\n",
    "    \"s2-go to s3\": 0.5,\n",
    "    \"s3-go to s4\": 0.5,\n",
    "    \"s3-go to s5\": 0.5,\n",
    "    \"s4-go to s5\": 0.5,\n",
    "    \"s4-random transit\": 0.5,\n",
    "}\n",
    "\n",
    "Pi_2 = {\n",
    "    \"s1-keep s1\": 0.6,\n",
    "    \"s1-go to s2\": 0.4,\n",
    "    \"s2-go to s1\": 0.3,\n",
    "    \"s2-go to s3\": 0.7,\n",
    "    \"s3-go to s4\": 0.5,\n",
    "    \"s3-go to s5\": 0.5,\n",
    "    \"s4-go to s5\": 0.1,\n",
    "    \"s4-random transit\": 0.9,\n",
    "}\n",
    "\n",
    "def join(str1, str2):\n",
    "    return str1 + '-' + str2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP中每个状态价值分别为\n",
      " [[-1.22555411]\n",
      " [-1.67666232]\n",
      " [ 0.51890482]\n",
      " [ 6.0756193 ]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "P_from_mdp_to_mrp = [\n",
    "    [0.5, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.5, 0.5],\n",
    "    [0.0, 0.1, 0.2, 0.2, 0.5],\n",
    "    [0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "]\n",
    "\n",
    "P_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)\n",
    "R_from_mdp_to_mrp = [-0.5, -1.5, -1.0, 5.5, 0]\n",
    "\n",
    "V = compute(P_from_mdp_to_mrp, R_from_mdp_to_mrp, gamma, 5)\n",
    "print(\"MDP中每个状态价值分别为\\n\", V)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating state value using Monte Carlo method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Use policy $\\pi$ to sample sequence(episodes)\n",
    "$$s_0^{(i)} \\xrightarrow[]{a_0^{(i)}} r_0^{(i)},s_1^{(i)}\\xrightarrow[]{a_1^{(i)}} r_1^{(i)},s_2^{(i)}\n",
    "\\xrightarrow[]{a_2^{(i)}} \\dots  \n",
    "\\xrightarrow[]{a_{T-1}^{(i)}} r_{T-1}^{(i)}, s_T^{(i)}$$\n",
    "\n",
    "Step 2: update counter and state value use:\n",
    "\n",
    "$N(s) \\rightarrow N(s) + 1$\n",
    "\n",
    "$V(s) \\rightarrow V(s) + \\frac{1}{N(s)}(G - V(S))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(MDP, Pi, timestamp_max, number):\n",
    "    S,A,P,R,gamma = MDP\n",
    "    episodes = []\n",
    "    for _ in range(number):\n",
    "        episode = []\n",
    "        timestamp = 0\n",
    "        s = S[np.random.randint(4)]\n",
    "        while s != \"s5\" and timestamp <= timestamp_max:\n",
    "            timestamp += 1\n",
    "            rand, temp = np.random.rand(), 0\n",
    "            for a_opt in A:\n",
    "                temp += Pi.get(join(s, a_opt), 0)\n",
    "                if temp > rand:\n",
    "                    a = a_opt\n",
    "                    r = R.get(join(s,a), 0)\n",
    "                    break\n",
    "            \n",
    "            rand,temp = np.random.rand(), 0\n",
    "            for s_opt in S:\n",
    "                temp += P.get(join(join(s,a),s_opt),0)\n",
    "                if temp > rand:\n",
    "                    s_next = s_opt\n",
    "                    break\n",
    "            episode.append((s,a,r,s_next))\n",
    "            s = s_next\n",
    "        episodes.append(episode)\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first episode\n",
      " [('s4', 'go to s5', 10, 's5')]\n",
      "second episode\n",
      " [('s3', 'go to s4', -2, 's4'), ('s4', 'go to s5', 10, 's5')]\n",
      "fifth episode\n",
      " [('s1', 'go to s2', 0, 's2'), ('s2', 'go to s3', -2, 's3'), ('s3', 'go to s4', -2, 's4'), ('s4', 'go to s5', 10, 's5')]\n"
     ]
    }
   ],
   "source": [
    "episodes = sample(MDP, Pi_1, 20, 5)\n",
    "print('first episode\\n', episodes[0])\n",
    "print('second episode\\n', episodes[1])\n",
    "print('fifth episode\\n', episodes[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monte carlo approximation for value funtion is\n",
      " {'s1': -1.2140412627671602, 's2': -1.6785753861863835, 's3': 0.5182357206312826, 's4': 6.134647097572852, 's5': 0}\n"
     ]
    }
   ],
   "source": [
    "def MC(episodes, V, N, gamma):\n",
    "    for episode in episodes:\n",
    "        G = 0\n",
    "        for i in range(len(episode) - 1, -1, -1):\n",
    "            (s , a, r, s_next) = episode[i]\n",
    "            G = r + gamma * G\n",
    "            N[s] = N[s] + 1\n",
    "            V[s] = V[s] + (G - V[s]) / N[s]\n",
    "\n",
    "timestep_max = 20\n",
    "episodes = sample(MDP, Pi_1, timestep_max, 10000)\n",
    "gamma = 0.5\n",
    "V = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "N = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "MC(episodes, V, N, gamma)\n",
    "print(\"monte carlo approximation for value funtion is\\n\", V)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupancy Measure \n",
    "\n",
    "we can define state visitation distribution as: \n",
    "$$v^\\pi (s) = (1 - \\gamma) \\sum_{t = 0}^\\infty \\gamma^t P_t^\\pi (s)$$\n",
    "where $P_t^\\pi (s)$ denote the the probability of state s at time t under policy $\\pi$, state visitation distribution has following properity:\n",
    "$$v^\\pi(s') = (1 - \\gamma)v_0(s') + \\gamma \\int P(s'|s,a)\\pi(a,s)v^\\pi(s)dsda$$\n",
    "\n",
    "the occupancy meansure which describe the probability that state-action pair (s,a) is visited can be defined as:\n",
    "$$\\rho^\\pi(s,a) = (1 - \\gamma) \\sum_{t = 0}^\\infty \\gamma^t P_t^\\pi(s)\\pi(a|s)$$\n",
    "\n",
    "the state visitation distribution and occupancy measure has following relation:\n",
    "$$\\rho^\\pi(s,a) = v^\\pi(s) \\pi(a|s)$$\n",
    "\n",
    "In the sametime we have following theorem:\n",
    "\n",
    "$\\rho^{\\pi_1} = rho^{\\pi_2} \\Longleftrightarrow \\pi_1 = \\pi_2$\n",
    "\n",
    "the only policy generating occupancy measure $\\rho$ is $\\pi_\\rho = \\frac{\\rho(s,a)}{\\sum_{a'} \\rho(s,a')}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11256503224375107 0.22837363293502716\n"
     ]
    }
   ],
   "source": [
    "def occupancy(episodes, s,a,timestep_max,gamma):\n",
    "    rho = 0\n",
    "    total_times = np.zeros(timestep_max) #record number of time t is visit \n",
    "    occur_times = np.zeros(timestep_max) #reord number of time (s_t, a_t) = (s,a)\n",
    "\n",
    "    for episode in episodes:\n",
    "        for i in range(len(episode)):\n",
    "            (s_opt, a_opt, r, s_next) = episode[i]\n",
    "            total_times[i] += 1\n",
    "            if s == s_opt and a == a_opt:\n",
    "                occur_times[i] += 1\n",
    "\n",
    "    for i in reversed(range(timestep_max)):\n",
    "        if total_times[i]:\n",
    "            rho += gamma**i * occur_times[i] / total_times[i]\n",
    "\n",
    "    return (1 - gamma) * rho\n",
    "\n",
    "gamma = 0.5\n",
    "timestep_max = 1000\n",
    "\n",
    "episodes_1 = sample(MDP, Pi_1, timestep_max, 1000)\n",
    "episodes_2 = sample(MDP, Pi_2, timestep_max, 1000)\n",
    "rho_1 = occupancy(episodes_1, \"s4\", \"random transit\", timestep_max, gamma)\n",
    "rho_2 = occupancy(episodes_2, \"s4\", \"random transit\", timestep_max, gamma)\n",
    "print(rho_1, rho_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2 | packaged by conda-forge | (main, Feb 17 2025, 14:02:48) [Clang 18.1.8 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ae41100f6d314d5b1c73ee87cbd1486b39c12788d7652902c395b0e193dba05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
